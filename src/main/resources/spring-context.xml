<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:hdp="http://www.springframework.org/schema/hadoop"
       xmlns:batch="http://www.springframework.org/schema/batch"
       xmlns:context="http://www.springframework.org/schema/context"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
       http://www.springframework.org/schema/hadoop http://www.springframework.org/schema/hadoop/spring-hadoop.xsd
       http://www.springframework.org/schema/batch http://www.springframework.org/schema/batch/spring-batch-2.1.xsd
       http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd">

    <context:property-placeholder  location="app.properties"/>

    <!-- <hdp:configuration id="hadoopConfiguration" properties-location="classpath:app.properties"/> -->

    <hdp:configuration id="hadoopConfiguration"/>

    <hdp:pig-factory exec-type="LOCAL" configuration-ref="hadoopConfiguration" />

    <batch:job-repository id="jobRepository"
                          data-source="dataSource"
                          transaction-manager="transactionManager"
                          isolation-level-for-create="SERIALIZABLE"
                          table-prefix="batch_"
                          max-varchar-length="1000"
            />

    <!-- Required to re-run jobs (access to history) -->
    <bean id="jobExplorer" class="org.springframework.batch.core.explore.support.JobExplorerFactoryBean">
        <property name="dataSource" ref="dataSource"/>
    </bean>

    <bean id="jobLauncher" class="org.springframework.batch.core.launch.support.SimpleJobLauncher">
        <property name="jobRepository" ref="jobRepository"/>
    </bean>

    <hdp:pig-template/>

    <!-- Required to get access to jobParameters from Hadoop tasklets -->
    <bean class="org.springframework.batch.core.scope.StepScope">
        <property name="proxyTargetClass" value="true"/>
    </bean>

    <!-- Batch -->
    <batch:job id="pigJob" job-repository="jobRepository">
        <batch:step id="prepareData" next="processData">
            <batch:tasklet ref="prepare-script-tasklet" />
        </batch:step>
        <batch:step id="processData">
            <batch:tasklet ref="aggByUrl"/>
            <batch:next on="FAILED" to="finishProcessing"/>
            <batch:next on="*" to="finishProcessing"/>
        </batch:step>
        <batch:step id="finishProcessing" next="endProcessing">
            <batch:tasklet ref="finish-script-tasklet" />
        </batch:step>
        <!-- step will be executed only on fail -->
        <batch:step id="endProcessing">
            <batch:tasklet ref="log-end" />
        </batch:step>
    </batch:job>

    <!-- groovy script that push data to timed directory (preparation step)
     Scope 'step' is obligated to provide access to jobParameters
     -->
    <hdp:script-tasklet id="prepare-script-tasklet" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            // fs - org.apache.hadoop.fs.FileSystem
            // fsh - org.springframework.data.hadoop.fs.FsShell instance based on 'hadoopConfiguration' bean
            // distcp	org.springframework.data.hadoop.fs.DistributedCopyUtil	Programmatic access to DistCp
            // jobParamaters = job parameters from spring batch

            println "work hour is  #{jobParameters['WorkingHour']}"

            // copy regular file to appropriate hour
            distcp.copy("${etl.logs.input}", "${pig.script.input.prefix}#{jobParameters['WorkingHour']}/")
        </hdp:script>
    </hdp:script-tasklet>


    <!-- push data to predefined directory; if this directory exist they will be removed -->
    <hdp:script-tasklet id="finish-script-tasklet" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            println "Publishing result to ${etl.logs.output} (will be overwritten)"

            // remove dir if exist
            if (fsh.test("${etl.logs.output}")) {
                 fsh.rmr("${etl.logs.output}")
            }
            // copy hour to general output
            distcp.copy("${pig.script.output.prefix}#{jobParameters['WorkingHour']}/", "${etl.logs.output}")
        </hdp:script>
    </hdp:script-tasklet>

    <!-- just simple example of last job in flow -->
    <hdp:script-tasklet id="log-end" scope="step">
        <hdp:script language="groovy" evaluate="ALWAYS">
            println "Reach the end of flow for ${pig.script.input.prefix}#{jobParameters['WorkingHour']}/"
        </hdp:script>
    </hdp:script-tasklet>

    <!-- pig job to aggregate log by some values -->
    <bean id="aggByUrl" class="pig.experiments.pigprocessors.ExecuteGeneralScript" >
        <property name="pigTemplate" ref="pigTemplate" />
        <property name="sciptName" value="classpath:pig/slow_pages.pig" />
        <property name="inputPath" value="${pig.script.input.prefix}" />
        <property name="outputPat" value="${pig.script.urlagg.output.prefix}" />
    </bean>

    <bean id="dataSource" class="org.enhydra.jdbc.pool.StandardPoolDataSource"
          destroy-method="shutdown">
        <constructor-arg index="0">
            <ref bean="outConnectionDataSource"/>
        </constructor-arg>
        <property name="maxSize" value="10"/>
        <property name="minSize" value="5"/>
    </bean>

    <!-- datasources configurations -->
    <bean id="outConnectionDataSource"
          class="org.enhydra.jdbc.standard.StandardConnectionPoolDataSource"
          destroy-method="shutdown">
        <property name="driverName" value="${db.out.driver}"/>
        <property name="url" value="${db.out.connection}"/>
    </bean>

    <bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
        <property name="dataSource" ref="dataSource"/>
    </bean>

</beans>